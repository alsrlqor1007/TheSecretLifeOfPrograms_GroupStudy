## 특성추출
<hr>

특성 추출 알고리즘은 많다. 그 중 `휴 변환` 같은 것은 선이나 원 같은 기하학적인 모양을 추출하기에 좋다.

<img src="https://user-images.githubusercontent.com/91880235/191030254-9363cb6f-c440-4d01-8830-8b50e4d5b016.png" width=500>

예시의 `고양이-미트로프` 문제의 경우 기하학적인 모양을 찾는 것이 아니기에 휴 변환이 유용하진 않다.

이미지를 스캔하면서 테두리를 찾고, 테두리를 추적해서 물체를 추출하자. 테두리가 서로 교차하면 가장 짧은 경로를 찾는다. 이제 스팸 감지 예제에서 했던 일들 다시 한다. 

<img src="https://user-images.githubusercontent.com/91880235/191030401-93a47dcb-c50a-43fc-9c6c-8c31a64a2fb4.png" width=500>

이 특성들을 분류기에 넣는다. `+`는 이런 특성이 있는 이미지가 결과에 부합하는 특성이라는 뜻이고, `-`는 결과에 반하는 특성이라는 뜻이다. 

특성을 분류하는 단계는 아래와 같다.

<img src="https://user-images.githubusercontent.com/91880235/191030449-048b7b12-e255-435d-9822-5ae595fbfce9.png" width=500>





<br>

## 인공 신경망
<hr>

어떤 수준에서는 물체를 표현하기 위해 사용하는 데이터가 어떤 종류인지는 실제로 문제가 되지 않는다. 이 세상에 존재하는 엄청난 변형들을 처리할 수 있어야 한다. 인간과 마찬가지로 컴퓨터도 입력을 바꿀 수는 없다. 따라서 다양성에 대응할 수 있는 더 나은 분류기가 필요하다. 

인공지능의 사용 접근 방법 중에는 인간의 행동을 흉내내려는 노력이 있었다. 
인간의 경우 `뉴런`이 큰 역할을 한다. 이를 논리게이트와 아날로그 비교기를 합한 것으로 생각할 수 있다.
<img src="https://user-images.githubusercontent.com/91880235/191033329-135a1402-eff5-4be5-9350-59f9a9910e44.png" width=500 height=200>

`수상돌기`는 `입력`이고 `축삭`은 `출력`이며, `축삭종말`은 축삭과 다른 뉴런을 연결해주는 것에 지나지 않는다. 뉴런을 게이트처럼 단순화하면 다음 이미지와 같다.

각 수상돌기의 값을 어떤 가중치로 곱한 다음, 모든 가중치가 곱해진 값을 더한다. 이 과정은 `베이즈 분류`와 비슷한데, 전체를 더한 값이 활동 전위보다 낮으면 비교기의 출력이 false, 아니라면 true가 된다.
비교기 출력이 true이면 뉴런이 출력 플립플롭을 true로 설정해서 활성화되는 식이다.

최초의 인공 뉴런을 만드려는 시도는 미국 심리학자인 `프랭크 로젠블랏`의 `퍼셉트론`이었다.

허나 퍼셉트론은 입력과 출력이 2진수라 0 아니면 1밖에 올 수 없다.
<img src="https://user-images.githubusercontent.com/91880235/191035247-66d88fe1-7c20-48fa-8a7b-63527e84129f.png" width=300 height=150>

이후 기존 퍼셉트론이 갖고있는 여러가지 문제를 해결한 다중 계층 신경망이 만들어졌다.

각 계층에서 만들어진 출력을 다음 계층으로만 보내기 떄문에 이런 신경망을 `앞먹임 신경망(feedforward network)`이라고 부른다. 가운데는 원하는 만큼 `은닉 계층(hidden layer)이 들어갈 수 있다. 

<img src="https://user-images.githubusercontent.com/91880235/191033475-f0753cf5-2b92-41f2-af50-65c4f6ce273a.png" width=500 height=200>

신경과학자들은 수상돌기의 가중치가 결정되는 방법을 몰랐고, 이를 해결하기 위해 비교기에 `시그모이드 함수`를 적용하여 `시그모이드 뉴런`을 탄생시켰다. 

시그모이드 함수는 S자 모양 곡선인 함수를 부르는 멋진 이름일 뿐인데, 아래의 두 함수는 아날로그와 디지털을 설명할 때 나왔던 함수와 거의 비슷하다. 

<img src="https://user-images.githubusercontent.com/91880235/191033498-0d98e644-de21-48b8-ba5c-fad5776049ef.png" width=300 height=100>

시그모이드 뉴런에서는 `역전파(backpropagation)` 기법을 사용해 신경망의 가중치를 결정할 수 있다. 
이 아이디어는 고양이의 특성처럼 이미 알고있는 대상에 대한 입력을 제공하는 것이다. 우리는 고양이임을 알고 있으므로 출력이 1이거나 1에 아주 가까운 값이어야 한다는 사실을 알기에 예상 출력값에서 실제 출력값을 뺀 값인 오류 함수를 계산할 수 있다. 그 후 오류 함수의 값을 가능한 한 0이 되도록 가중치를 조정한다.


<img src="https://user-images.githubusercontent.com/91880235/191033521-61e6af47-61ef-4183-a8ee-0ecdb02c1e90.png" width=500 height=200>

인공 신경망의 큰 문제는 나쁜 훈련 데이터에 의해 신경망이 '중독' 될 수 있다는 점이다. 아이일 때 텔레비전 프로그램을 너무 많이 본 어른이 나중에 어떤 이상 행동을 보일지 알 수 없는 것처럼 머신러닝 시스템도 마찬가지다. 

신경망에서 핵심은 이들이 아주 능력이 많은 분류기라는 점이다. 엄청난 양의 입력 데이터를 가지고, 우리가 원하는 대로 입력을 기술하는 훨씬 적은 양의 출력으로 변환하도록 신경망을 훈련시킬 수 있다. 

### 아키텍처에 기반한 인공신경망 분류
- `합성곱 신경망(Convolutional Neural Network)`: 인간의 시신경 구조를 모방해 만들어진 인공신경망 알고리즘. 다수의 Convolutional Layer으로 부터 특징맵(Feature map)을 추출하고 서브샘플링(Subsampling)을 통해 차원을 축소하여 특징맵에서 중요한 부분만을 가져온다. 이미지 분류, semantic segmentation, optical flow등등 대부분의 컴퓨터 비전(computer vision)분야에서 필수적으로 사용되는 기술이며, 기존의 MLP(다층 퍼셉트론)에 비해 적은 연산량과 높은 성능을 보여줘 각광받고 있다.
- `순환 신경망(Recurrent Neural Network)`: 뉴런의 출력이 다시 입력으로 feedback되는 재귀적인 연결 구조를 갖는 신경망. 다층 퍼셉트론 신경망은 입력이 출력 방향으로만 활성화되고 은닉 뉴런이 과거의 정보를 기억하지 못한다는 단점이 있다. 이러면 입력이 들어온 문맥을 기억할 수 없다. 이런 단점은 시계열분석관련 문제에서 매우 해롭다. RNN은 이런 단점을 해결했다.
기존의 DNN(Deep Neural Networks)의 경우 각 layer마다 parameter들이 독립적이었으나, RNN은 이를 공유하고 있다. 따라서 현재의 출력 결과는 이전 time step의 결과에 영향을 받으며, hidden layer는 일종의 메모리 역할을 하게 된다.
- `장단기 기억 신경망(Long Short Term Memory, LSTM)`: 1997년 나온 RNN의 개선판. 학습이 오래 지속될 경우 초기 학습한 내용이 잊혀진다는 단점이 있는데 이를 개선한 아키텍처다. 
- `Gated Recurrent Unit(GRU)`: 크게 보면 RNN의 일종이며 LSTM의 개선판이라고 할 수 있다. 2014년 뉴욕대학교 조경현 교수가 발표한, LSTM의 장기기억능력은 보존하면서 연산은 적은 모델이다.


<br>

## 참고
<hr>

- [인공신경망이란 무엇인가?](https://blog.lgcns.com/1359)
- [나무위키 - 인공신경망](https://namu.wiki/w/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D)


